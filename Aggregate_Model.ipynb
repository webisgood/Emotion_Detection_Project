{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1f-oepOMQs1DHi6Q3FxFvFjxi-j6t20CO","timestamp":1700890707681},{"file_id":"1xjIi1YT55UFTClhkQPd9sPKsHgTH3QjJ","timestamp":1700760959698}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 0) Necessary Imports & Installations"],"metadata":{"id":"ibkz7wBR_F1Q"}},{"cell_type":"code","source":["pip install pydub"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ndx5oaPqHcSd","executionInfo":{"status":"ok","timestamp":1700911721429,"user_tz":-330,"elapsed":8080,"user":{"displayName":"TARUN RAJKUMAR","userId":"12274055638596808511"}},"outputId":"7a7676d6-1c16-4a0d-ed4a-a4868f56c3b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pydub\n","  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n","Installing collected packages: pydub\n","Successfully installed pydub-0.25.1\n"]}]},{"cell_type":"code","source":["# Import the necessary libraries\n","\n","import tensorflow as tf\n","import cv2\n","import os\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import librosa\n","import librosa.display\n","from IPython.display import Audio\n","\n","import moviepy.editor\n","\n","from pydub import AudioSegment\n","\n","from google.colab.patches import cv2_imshow"],"metadata":{"id":"5QSYi-XLGoWs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1) Loading the Dataset"],"metadata":{"id":"ACePnnsQ-pwQ"}},{"cell_type":"code","source":["# Mount the drive into this notebook\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GyDTuTejGfdm","executionInfo":{"status":"ok","timestamp":1701162406239,"user_tz":-330,"elapsed":30796,"user":{"displayName":"THATHAPUDI SANJEEV PAUL JOEL","userId":"06949672390030411103"}},"outputId":"b10fb027-e4ee-49e6-eecf-8d7284e2763e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Unzip the dataset and extract the content\n","\n","import zipfile\n","\n","zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/NNFL Project/Datasets/Video_Speech_Actor_01.zip\")\n","zip_ref.extractall()\n","zip_ref.close()\n","\n","zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/NNFL Project/Datasets/facial_emotion_dataset.zip\")\n","zip_ref.extractall()\n","zip_ref.close()"],"metadata":{"id":"iyQz3w2AG7D5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class_labels = [\"angry\", \"disgusted\", \"fearful\",\n","           \"happy\", \"neutral\", \"sad\", \"surprised\"]\n","\n","num_classes = len(class_labels)"],"metadata":{"id":"iSxZ-CRIff4t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_and_preprocess_image(file_path, target_size):\n","    image = cv2.imread(file_path)\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB format\n","    image = cv2.resize(image, target_size)  # Resize to a common size\n","    return image\n","\n","\n","data_dir = '/content/emotion_dataset/train'\n","target_size = (64, 64)  # Adjust the size as needed"],"metadata":{"id":"o2Gjj13-fhlB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","train_data = []\n","train_labels = []\n","\n","for class_name in os.listdir(data_dir):\n","    class_dir = os.path.join(data_dir, class_name)\n","    class_id = class_labels.index(class_name)  # You need to define class_labels\n","\n","    for image_file in os.listdir(class_dir):\n","        image_path = os.path.join(class_dir, image_file)\n","        image = load_and_preprocess_image(image_path, target_size)\n","        train_data.append(image)\n","        train_labels.append(class_id)\n","\n","train_data = np.array(train_data)\n","train_labels = np.array(train_labels)\n"],"metadata":{"id":"S2cThntYgGgD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2) Extracting Video and Audio from the samples"],"metadata":{"id":"15WRJIgn-zAH"}},{"cell_type":"code","source":["# Function to capture individual frames present in a video\n","\n","def FrameCapture(path):\n","  frames = []\n","\n","  vidObj = cv2.VideoCapture(path)\n","\n","  count = 0\n","\n","  success = 1\n","\n","  while success:\n","    success, image = vidObj.read()\n","\n","    if success == False:\n","      break\n","\n","    # Converts the image frame into grayscale\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","\n","    # Resizes the image to the dimensions as required by the pre-trained model\n","    image = cv2.resize(image, (64, 64))\n","\n","    # Further necessary image transformations\n","    image = np.dstack([image]*3 )\n","    image = np.expand_dims(image, axis=0)\n","\n","    # To store each frame into one large 'frames' list\n","    frames.append(image)\n","    count += 1\n","\n","  return frames"],"metadata":{"id":"rZZqpkjWSHgb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to extract audio from a given audio-visual file\n","\n","import moviepy.editor as mp\n","\n","def ExtractAudio(path):\n","\t# Insert Local Video File Path\n","\tclip = mp.VideoFileClip(path)\n","\n"," \t# Insert Local Audio File Path\n","\tclip.audio.write_audiofile(path.split(\".\")[0] + \".mp3\")\n","\n","\t# Paths for mp3 and wav files\n","\tinput_file = (path.split(\".\")[0] + \".mp3\")\n","\toutput_file = path.split(\".\")[0] + \".wav\"\n","\n","\t# convert mp3 file to wav file\n","\tsound = AudioSegment.from_mp3(input_file)\n","\tsound.export(output_file, format = \"wav\")\n","\n","# Extract video from each audio file\n","\n","for i in range(len(paths)):\n","\tExtractAudio(paths[i])"],"metadata":{"id":"-jo1Qswi_5yv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to extract mfcc features from audio samples\n","\n","def extract_mfcc(filename):\n","    y, sr = librosa.load(filename, duration=3, offset=0.5)\n","    mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)\n","    return mfcc"],"metadata":{"id":"8cVgjOgBHx5G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Iterate through each directory and store the path of each file\n","\n","paths = []\n","labels = []\n","for dirname, _, filenames in os.walk('/content/Actor_01'):\n","    counter = 0\n","    for filename in filenames:\n","        paths.append(os.path.join(dirname, filename))\n","        label = filename.split('_')[-1]\n","        label = label.split('.')[0]\n","        labels.append(label.lower())\n","        counter = counter + 1\n","        if(counter == 60):\n","            break\n","print('Dataset is Loaded')"],"metadata":{"id":"LO7YfbdTPEWO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700913370595,"user_tz":-330,"elapsed":383,"user":{"displayName":"TARUN RAJKUMAR","userId":"12274055638596808511"}},"outputId":"7a4d4fea-7d43-48df-c229-25b7abd5d6ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset is Loaded\n"]}]},{"cell_type":"code","source":["# Store the frames of all the 60 videos\n","\n","frames_dir = []\n","\n","for i in range(len(paths)):\n","  frames = FrameCapture(paths[i])\n","\n","  frames_dir.append(frames)\n","\n","len(frames_dir)"],"metadata":{"id":"0rtWE_xX_ihG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700913434383,"user_tz":-330,"elapsed":56187,"user":{"displayName":"TARUN RAJKUMAR","userId":"12274055638596808511"}},"outputId":"3950e8df-023f-47c5-c382-eff302f1fe7b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["60"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["max_frames = max(len(frames_dir[i]) for i in range(len(frames_dir)))\n","max_frames"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o8B5RW7GVrjN","executionInfo":{"status":"ok","timestamp":1700913899051,"user_tz":-330,"elapsed":6,"user":{"displayName":"TARUN RAJKUMAR","userId":"12274055638596808511"}},"outputId":"4bf5edb0-379d-4ba1-e639-089b468170b2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["149"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["np.array(frames_dir[0]).shape\n","# empty_frames = np.zeros((19, 1, 64, 64, 3))\n","\n","# np.concatenate((np.array(frames_dir[0]), empty_frames)).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T9QF1C8-YfpA","executionInfo":{"status":"ok","timestamp":1700914554106,"user_tz":-330,"elapsed":9,"user":{"displayName":"TARUN RAJKUMAR","userId":"12274055638596808511"}},"outputId":"204b82ed-299a-42e1-f60c-5f4e03323bc7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(130, 1, 64, 64, 3)"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["frames_dir_new = []\n","\n","for i in range(len(frames_dir)):\n","  frames = np.array(frames_dir[i])\n","  empty_frames = np.zeros((max_frames - len(frames_dir[i]), 1, 64, 64, 3))\n","\n","  frames_dir_new.append(np.concatenate((frames, empty_frames)))\n","\n","frames_dir = frames_dir_new\n","frames_dir[0].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o4rHvGQCW5nB","executionInfo":{"status":"ok","timestamp":1700914888177,"user_tz":-330,"elapsed":7,"user":{"displayName":"TARUN RAJKUMAR","userId":"12274055638596808511"}},"outputId":"16ca6dbe-0dfe-4f52-c911-af52bd456b11"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(149, 1, 64, 64, 3)"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["# Function to make a dataframe of audio file paths\n","\n","import pandas as pd\n","\n","df = pd.DataFrame()\n","\n","Y = []\n","\n","for i in range(len(paths)):\n","  Y.append(paths[i].split(\".\")[0] + \".wav\")\n","\n","df['speech'] = Y"],"metadata":{"id":"5tXhJvPt_vpj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function call to extract the mfcc features from each audio file path present in the dataframe df\n","\n","X_mfcc = df['speech'].apply(lambda x: extract_mfcc(x))\n","\n","# Converting the dataframe list into a numpy array\n","\n","X = [x for x in X_mfcc]\n","X = np.array(X)\n","X.shape\n","\n","# Necessary size transformations for passing to the model\n","\n","X = np.expand_dims(X, -1)\n","\n","## Uncomment this when testing out a single sound sample\n","# X = np.expand_dims(X, axis=0)\n","\n","X.shape"],"metadata":{"id":"ytT3IPXWH3_z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3) Model Definition"],"metadata":{"id":"wAuJ8JAL-_be"}},{"cell_type":"code","source":["# Loading pre-trained models\n","\n","model_images = tf.keras.models.load_model('/content/drive/MyDrive/NNFL Project/Models/final_model_Custom.h5')\n","\n","model_speech = tf.keras.models.load_model('/content/drive/MyDrive/NNFL Project/Models/final_model_speechRecognition.h5')"],"metadata":{"id":"3G_exBEWUkca"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_images\n"],"metadata":{"id":"7fmQiy8DAv0d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700911906585,"user_tz":-330,"elapsed":374,"user":{"displayName":"TARUN RAJKUMAR","userId":"12274055638596808511"}},"outputId":"b134bba8-1a37-4809-cfee-734b2f312ebf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<keras.src.engine.sequential.Sequential at 0x7bed35287f40>"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# Model for images\n","model1 = models.Sequential([\n","    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n","    layers.MaxPooling2D((2, 2)),\n","    layers.Conv2D(64, (3, 3), activation='relu'),\n","    layers.MaxPooling2D((2, 2)),\n","    layers.Conv2D(64, (3, 3), activation='relu'),\n","    layers.Flatten(),\n","    layers.Dense(64, activation='relu')\n","])\n","\n","# Model for audio\n","model2 = Sequential([\n","    LSTM(256, return_sequences=False, input_shape=(40,1)),\n","    Dropout(0.2),\n","    Dense(128, activation='relu'),\n","    Dropout(0.2),\n","    Dense(64, activation='relu')\n","])"],"metadata":{"id":"p_NQrEUofSXJ"},"execution_count":null,"outputs":[]}]}